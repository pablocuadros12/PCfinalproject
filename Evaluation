# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s57aJyUZyVPkU30faAUtPyi02rdocIzv
"""

# Install the ucimlrepo module
!pip install ucimlrepo

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler

# Import dataset using ucimlrepo
from ucimlrepo import fetch_ucirepo

# Fetch dataset
cdc_diabetes_health_indicators = fetch_ucirepo(id=891)

# Data (as pandas dataframes)
X = cdc_diabetes_health_indicators.data.features
y = cdc_diabetes_health_indicators.data.targets

# Combine features and target into a single dataframe
data = pd.concat([X, y], axis=1)

# Define logical subsequences
demographic_health_features = ['Sex', 'Age', 'GenHlth', 'Education', 'Income']
lifestyle_behavior_features = ['Smoker', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump']
medical_history_features = ['HighBP', 'HighChol', 'HeartDiseaseorAttack', 'Stroke', 'CholCheck']

# Split the dataset into subsequences
X_demo_health = data[demographic_health_features]
X_lifestyle = data[lifestyle_behavior_features]
X_med_history = data[medical_history_features]

# Combine subsequences for different combinations
X_demo_health_lifestyle = pd.concat([X_demo_health, data[lifestyle_behavior_features]], axis=1)
X_demo_health_med_history = pd.concat([X_demo_health, data[medical_history_features]], axis=1)
X_lifestyle_med_history = pd.concat([X_lifestyle, data[medical_history_features]], axis=1)
X_all = data.drop(columns=['Diabetes_binary'])  # All features

# List of continuous features
continuous_features = ['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']

# Standardize continuous features separately for each subset
def fit_and_transform_subset(df, continuous_features):
    scaler = StandardScaler()
    available_features = [feature for feature in continuous_features if feature in df.columns]
    if available_features:  # Only fit and transform if there are continuous features present
        df[available_features] = scaler.fit_transform(df[available_features])
    return df

X_demo_health = fit_and_transform_subset(X_demo_health, continuous_features)
X_lifestyle = fit_and_transform_subset(X_lifestyle, continuous_features)
X_med_history = fit_and_transform_subset(X_med_history, continuous_features)
X_demo_health_lifestyle = fit_and_transform_subset(X_demo_health_lifestyle, continuous_features)
X_demo_health_med_history = fit_and_transform_subset(X_demo_health_med_history, continuous_features)
X_lifestyle_med_history = fit_and_transform_subset(X_lifestyle_med_history, continuous_features)
X_all = fit_and_transform_subset(X_all, continuous_features)

# Define the models to compare
models = {
    "Logistic Regression": LogisticRegression(max_iter=10000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100)
}

# Function to evaluate models using cross-validation
def evaluate_models(X, y, models):
    results = {}
    for model_name, model in models.items():
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
        results[model_name] = {
            "Mean ROC AUC": cv_scores.mean(),
            "Std Dev ROC AUC": cv_scores.std()
        }
    return results

# Evaluate models on each combination of subsequences
subsequence_combinations = {
    "Demographic and Health": X_demo_health,
    "Lifestyle and Behavior": X_lifestyle,
    "Medical History": X_med_history,
    "Demo Health + Lifestyle": X_demo_health_lifestyle,
    "Demo Health + Medical History": X_demo_health_med_history,
    "Lifestyle + Medical History": X_lifestyle_med_history,
    "All Features": X_all
}

# Evaluate models on each combination
results = {}
for name, X_subset in subsequence_combinations.items():
    print(f"\nEvaluating models on {name} features:")
    results[name] = evaluate_models(X_subset, y, models)
    for model_name, metrics in results[name].items():
        print(f"{model_name}: Mean ROC AUC = {metrics['Mean ROC AUC']:.2f}, Std Dev ROC AUC = {metrics['Std Dev ROC AUC']:.2f}")

"""## Evaluation of Different Feature Sets with Various Models

### Demographic and Health Features

| Model                | Mean ROC AUC | Std Dev ROC AUC |
|----------------------|--------------|-----------------|
| Logistic Regression  | 0.77         | 0.00            |
| Decision Tree        | 0.76         | 0.00            |
| Random Forest        | 0.76         | 0.00            |
| Gradient Boosting    | 0.78         | 0.00            |

### Lifestyle and Behavior Features

| Model                | Mean ROC AUC | Std Dev ROC AUC |
|----------------------|--------------|-----------------|
| Logistic Regression  | 0.62         | 0.00            |
| Decision Tree        | 0.62         | 0.00            |
| Random Forest        | 0.62         | 0.00            |
| Gradient Boosting    | 0.62         | 0.00            |

### Medical History Features

| Model                | Mean ROC AUC | Std Dev ROC AUC |
|----------------------|--------------|-----------------|
| Logistic Regression  | 0.75         | 0.00            |
| Decision Tree        | 0.75         | 0.00            |
| Random Forest        | 0.75         | 0.00            |
| Gradient Boosting    | 0.75         | 0.00            |

### Demographic Health + Lifestyle Features

| Model                | Mean ROC AUC | Std Dev ROC AUC |
|----------------------|--------------|-----------------|
| Logistic Regression  | 0.78         | 0.00            |
| Decision Tree        | 0.65         | 0.00            |
| Random Forest        | 0.71         | 0.00            |
| Gradient Boosting    | 0.78         | 0.00            |

### Demographic Health + Medical History Features

| Model                | Mean ROC AUC | Std Dev ROC AUC |
|----------------------|--------------|-----------------|
| Logistic Regression  | 0.80         | 0.00            |
| Decision Tree        | 0.75         | 0.00            |
| Random Forest        | 0.75         | 0.00            |
| Gradient Boosting    | 0.80         | 0.00            |

## Key Observations

- **Best Performance:** Logistic Regression and Gradient Boosting consistently performed well across most feature sets, particularly when combining demographic health and medical history features (ROC AUC = 0.80).
- **Lifestyle and Behavior Features:** These features resulted in the lowest performance across all models (ROC AUC = 0.62).
- **Std Dev ROC AUC:** All models had a standard deviation of 0.00 for the ROC AUC scores, indicating very consistent performance across cross-validation folds.

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import OneHotEncoder


# Define feature engineering function
def feature_engineering(df):
    # Example feature engineering (you can modify and add more as needed)
    df['BMI_category'] = pd.cut(df['BMI'], bins=[0, 18.5, 24.9, 29.9, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])

    # Perform one-hot encoding for BMI_category
    df = pd.get_dummies(df, columns=['BMI_category'], drop_first=True)

    return df

# Function to load and split data
def load_and_split_data():

    # Define features and target variable
    features_reg = ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity',
                    'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth',
                    'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']

    features_fe = ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity',
                   'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth',
                   'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income',
                   'BMI_category_Normal', 'BMI_category_Overweight', 'BMI_category_Obese']

    target = 'Diabetes_binary'

    # Perform feature engineering
    data_fe = feature_engineering(data.copy())

    # Regular dataset
    X_reg = data[features_reg]  # Replace with your actual features
    y_reg = data[target]

    # Feature-engineered dataset
    X_fe = data_fe.drop(columns=[target])  # Ensure target column is not included in features
    y_fe = data_fe[target]

    # Splitting the data (assuming you want to use the same train/test split as before)
    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)
    X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_fe, y_fe, test_size=0.2, random_state=42)

    return X_train_reg, X_test_reg, y_train_reg, y_test_reg, X_train_fe, X_test_fe, y_train_fe, y_test_fe

# Function to train and evaluate Gradient Boosting models
def train_and_evaluate_models(X_train_reg, X_test_reg, y_train_reg, y_test_reg, X_train_fe, X_test_fe, y_train_fe, y_test_fe):
    # Initialize Gradient Boosting Classifier
    gb_clf_reg = GradientBoostingClassifier(random_state=42)
    gb_clf_fe = GradientBoostingClassifier(random_state=42)

    # Train models
    gb_clf_reg.fit(X_train_reg, y_train_reg)
    gb_clf_fe.fit(X_train_fe, y_train_fe)

    # Predictions
    y_pred_reg = gb_clf_reg.predict(X_test_reg)
    y_pred_fe = gb_clf_fe.predict(X_test_fe)

    # Evaluation metrics for regular dataset model
    print("Regular Dataset Model:")
    print("Accuracy:", accuracy_score(y_test_reg, y_pred_reg))
    print("Precision:", precision_score(y_test_reg, y_pred_reg))
    print("Recall:", recall_score(y_test_reg, y_pred_reg))
    print("F1-score:", f1_score(y_test_reg, y_pred_reg))
    print(classification_report(y_test_reg, y_pred_reg))

    # Evaluation metrics for feature-engineered dataset model
    print("\nFeature-Engineered Dataset Model:")
    print("Accuracy:", accuracy_score(y_test_fe, y_pred_fe))
    print("Precision:", precision_score(y_test_fe, y_pred_fe))
    print("Recall:", recall_score(y_test_fe, y_pred_fe))
    print("F1-score:", f1_score(y_test_fe, y_pred_fe))
    print(classification_report(y_test_fe, y_pred_fe))

# Load and split data
X_train_reg, X_test_reg, y_train_reg, y_test_reg, X_train_fe, X_test_fe, y_train_fe, y_test_fe = load_and_split_data()

# Train and evaluate models
train_and_evaluate_models(X_train_reg, X_test_reg, y_train_reg, y_test_reg, X_train_fe, X_test_fe, y_train_fe, y_test_fe)

"""Regular Dataset Model:
Accuracy: 0.8675
Precision: 0.5636
Recall: 0.1742
F1-score: 0.2662
              precision    recall  f1-score   support
           0       0.88      0.98      0.93     43739
           1       0.56      0.17      0.27      6997
    accuracy                           0.87     50736
   macro avg       0.72      0.58      0.60     50736
weighted avg       0.84      0.87      0.84     50736

Feature-Engineered Dataset Model:
Accuracy: 0.8675
Precision: 0.5636
Recall: 0.1742
F1-score: 0.2662
              precision    recall  f1-score   support
           0       0.88      0.98      0.93     43739
           1       0.56      0.17      0.27      6997
    accuracy                           0.87     50736
   macro avg       0.72      0.58      0.60     50736
weighted avg       0.84      0.87      0.84     50736

Both models, trained on the regular dataset and the feature-engineered dataset, show identical performance metrics across accuracy, precision, recall, and F1-score. This suggests that the feature engineering applied did not lead to any improvement or degradation in model performance compared to using the raw dataset.

### Explanation for Choosing Logistic Regression Model with Demographic + Medical History Subset

#### Model Choice: Logistic Regression

We chose the logistic regression model using the demographic and medical history subset for the following reasons:

- **Simplicity and Interpretability**: Logistic regression is straightforward to implement and its results are easy to interpret. This is crucial in healthcare, where understanding how features impact predictions is important for trust and transparency.
- **Baseline Performance**: In our preliminary evaluations, logistic regression performed consistently well across various feature subsets, providing a strong baseline for comparison.
- **Efficiency**: Logistic regression is computationally efficient, making it suitable for large datasets and real-time applications.

#### Data Subset: Demographic + Medical History

The demographic and medical history subset includes the following features:

- **Demographic Information**: Age, Sex, Education, Income
- **Medical History**: HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, GenHlth, MentHlth, PhysHlth, DiffWalk

This subset was chosen because these features are typically strong indicators of diabetes risk and provide a comprehensive view of an individual's health status, enabling better prediction of diabetes onset.

#### Why ROC AUC Was Used to Evaluate Models

**ROC AUC** (Receiver Operating Characteristic Area Under the Curve) was used to evaluate the models for these reasons:

- **Balanced Evaluation**: ROC AUC provides a comprehensive metric that considers all classification thresholds, offering a balanced assessment of model performance.
- **Class Imbalance Handling**: It is particularly effective in handling imbalanced datasets, common in diabetes prediction, ensuring fair evaluation of the model's ability to detect minority classes.
- **Discrimination Power**: ROC AUC measures the model's ability to distinguish between positive (diabetic) and negative (non-diabetic) cases, critical for medical applications.
- **Comparative Analysis**: It allows for standardized comparison across multiple models and feature sets, helping identify the best-performing model.
- **Interpretability**: The visual ROC curve and AUC value are easy to interpret and communicate to stakeholders, making it a valuable tool for model performance evaluation in healthcare.

By using ROC AUC, we ensured a robust and fair evaluation of our logistic regression model, confirming its suitability for predicting diabetes with the chosen feature subset.

### Overall Summary

In summary, we opted for a logistic regression model using the demographic and medical history subset of the dataset due to its simplicity, interpretability, and strong baseline performance. The features in this subset are highly relevant for predicting diabetes, providing a comprehensive view of an individual's health status. ROC AUC was chosen as the evaluation metric to ensure a balanced, fair, and interpretable assessment of the model's performance. This combination ensures that our model is both effective and reliable for predicting diabetes, making it suitable for integration into a real-time web application.
"""
